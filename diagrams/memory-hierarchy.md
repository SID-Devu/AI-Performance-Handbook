# GPU Memory Hierarchy Diagram

```
┌────────────────────────────────────────────────────────────────────────┐
│                           GPU MEMORY HIERARCHY                          │
└────────────────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────────────────┐
│                              REGISTERS                                  │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ ┌─────────────┐       │
│  │  Thread 0   │ │  Thread 1   │ │  Thread 2   │ │  Thread N   │       │
│  │  VGPRs/SGPRs│ │  VGPRs/SGPRs│ │  VGPRs/SGPRs│ │  VGPRs/SGPRs│       │
│  └─────────────┘ └─────────────┘ └─────────────┘ └─────────────┘       │
│                                                                         │
│  Access: 0 cycles | Size: ~100KB per CU | Bandwidth: ~TB/s             │
└────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌────────────────────────────────────────────────────────────────────────┐
│                    LOCAL DATA SHARE (LDS) / SHARED MEMORY              │
│  ┌──────────────────────────────────────────────────────────────┐     │
│  │                    Per-Workgroup Memory                        │     │
│  │  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐       │     │
│  │  │Bank 0│ │Bank 1│ │Bank 2│ │Bank 3│ │ ...  │ │Bank31│       │     │
│  │  └──────┘ └──────┘ └──────┘ └──────┘ └──────┘ └──────┘       │     │
│  └──────────────────────────────────────────────────────────────┘     │
│                                                                         │
│  Access: ~20 cycles | Size: 64KB per CU | Bandwidth: ~12 TB/s          │
└────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌────────────────────────────────────────────────────────────────────────┐
│                             L1 CACHE                                    │
│  ┌──────────────────────────────────────────────────────────────┐     │
│  │                      Per-CU Cache                              │     │
│  │              Read-only | Write-through/around                  │     │
│  └──────────────────────────────────────────────────────────────┘     │
│                                                                         │
│  Access: ~50 cycles | Size: 16-32KB per CU | Bandwidth: ~2 TB/s        │
└────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌────────────────────────────────────────────────────────────────────────┐
│                             L2 CACHE                                    │
│  ┌──────────────────────────────────────────────────────────────┐     │
│  │                    Shared Across All CUs                       │     │
│  │  ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐      │     │
│  │  │Slice 0 │ │Slice 1 │ │Slice 2 │ │ ...    │ │Slice N │      │     │
│  │  └────────┘ └────────┘ └────────┘ └────────┘ └────────┘      │     │
│  └──────────────────────────────────────────────────────────────┘     │
│                                                                         │
│  Access: ~200 cycles | Size: 4-8MB | Bandwidth: ~3 TB/s                │
└────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌────────────────────────────────────────────────────────────────────────┐
│                     GLOBAL MEMORY (HBM/GDDR)                           │
│  ┌──────────────────────────────────────────────────────────────┐     │
│  │                     High Bandwidth Memory                      │     │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐             │     │
│  │  │ Stack 0 │ │ Stack 1 │ │ Stack 2 │ │ Stack 3 │             │     │
│  │  │  16GB   │ │  16GB   │ │  16GB   │ │  16GB   │             │     │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘             │     │
│  └──────────────────────────────────────────────────────────────┘     │
│                                                                         │
│  Access: ~500 cycles | Size: 32-192GB | Bandwidth: 1.5-3.5 TB/s        │
└────────────────────────────────────────────────────────────────────────┘


                          MEMORY COMPARISON TABLE

┌──────────────┬──────────────┬──────────────┬──────────────┬────────────┐
│   Memory     │   Latency    │     Size     │  Bandwidth   │   Scope    │
├──────────────┼──────────────┼──────────────┼──────────────┼────────────┤
│  Registers   │   0 cycles   │  ~100KB/CU   │   ~TB/s      │   Thread   │
│  LDS         │  ~20 cycles  │   64KB/CU    │   ~12 TB/s   │   Block    │
│  L1 Cache    │  ~50 cycles  │   16KB/CU    │   ~2 TB/s    │   CU       │
│  L2 Cache    │  ~200 cycles │    4-8MB     │   ~3 TB/s    │   GPU      │
│  HBM         │  ~500 cycles │   32-192GB   │  1.5-3.5 TB/s│   GPU      │
└──────────────┴──────────────┴──────────────┴──────────────┴────────────┘

```

## Key Optimization Strategies

1. **Maximize Register Usage**: Keep frequently accessed data in registers
2. **Use LDS for Shared Data**: Cache tiles that are accessed by multiple threads
3. **Coalesce Global Memory**: Ensure adjacent threads access adjacent memory
4. **Prefetch to L2**: Use large enough working sets to benefit from L2
5. **Minimize Host-Device Transfers**: Keep data on GPU when possible
